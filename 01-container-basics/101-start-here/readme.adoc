= Kubernetes - Setup Cloud9 Development Environment
:toc:
:icons:
:linkattrs:
:imagesdir: ../../resources/images


This section walks you through the creating a Kubernetes development environment using https://aws.amazon.com/cloud9/[AWS Cloud9].  This will provide you with a cloud-based integrated development environment (IDE) that will let you write, run, and debug containerized workloads using just a web browser.

== Create AWS Cloud9 Environment
=== AWS Cloud9 Console

We can create the Cloud9 development environment via CloudFormation.
This CloudFormation template will spin up the Cloud9 IDE, as well as configure the IDE environment for the rest of the workshop.

The CloudFormation template will use an existing VPC which has been pre-provisioned by AWS.

Click on the "Deploy to AWS" button and follow the CloudFormation prompts to begin.

[NOTE]
AWS Cloud9 is currently available in 5 regions, and EKS is currently available in 2 regions (us-east-1 and us-west-2).
Please choose the region closest to you.  If you choose a region for Cloud9 that does not support EKS, you need to create VPC resources and change environment variables. This configuration has not been tested.

|===

|Region | Launch template with an existing VPC
| *N. Virginia* (us-east-1)
a| image::./deploy-to-aws.png[link=https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=k8s-workshop&templateURL=https://205675256514-cfn-templates.s3.amazonaws.com/cloud9.json]
|===

Provide a *unique stack name* and Click *Next*. You can give Tags such as Key=Name, Value=k8s-workshop, and click *Next*. Make sure
to check *I acknowledge that AWS CloudFormation might create IAM resources with custom names* and click *Create*.

CloudFormation creates nested stacks and builds several resources that are required for this workshop. Wait until all the resources are created. Once the status for *k8s-workshop* changes to *CREATE_COMPLETE*,
you can open Cloud9 IDE. To open the Cloud9 IDE environment, click on the "Outputs" tab in CloudFormation Console and click on the "Cloud9IDE" URL.

image:cloudformation-output-tab.png[CloudFormation Output Tab]

You should see an environment similar to this:

image:cloud9-development-environment-welcome.png[]

=== Cloud9 Instance Role

The Cloud9 IDE needs to use the assigned IAM Instance profile. Open the "AWS Cloud9" menu, go to "Preferences", go to "AWS Settings", and disable "AWS managed temporary credentials" as depicted in the diagram here:

image:cloud9-disable-temp-credentials.png[]

=== Cloud9 Instance Setup

Once your Cloud9 is ready, clone this repo down to your Cloud9 environment.

`git clone https://github.com/dewjam/aws-workshop-for-kubernetes.git`

We have provided a "Utilities" Dockerfile which can be built that includes all
the tools you need to complete the workshop:

- kubectl _(the Kubernetes CLI)_
- heptio/authenticator _(for authentication to the EKS cluster)_
- AWS CLI
- sets appropriate environmental variables
- clones the workshop git repo into the container image

Build the Utilities image
```
docker build -t utilities aws-workshop-for-kubernetes/01-container-basics/101-start-here/utilities/
```

Run the Utilities image
```
docker run -it \
    -v ~/.aws/:/root/.aws \
    -v ~/.kube/:/root/.kube \
    -v ~/environment/aws-workshop-for-kubernetes:/aws-workshop-for-kubernetes \
    utilities
```

At this point you, you should be presented with a "$" prompt.  You are now ready
to continue on with the workshop!

[NOTE]
All shell commands _(starting with "$")_ throughout the rest of the workshop should be run in this tab. You may want to resize it upwards to make it larger.

:frame: none
:grid: none
:valign: top

[align="center", cols="3", grid="none", frame="none"]
|=====
|image:button-continue-standard.png[link=../102-your-first-cluster/]
|=====

Next Steps 

= AWS Container Immersion Day: Lab 1

![](media/image1.png){width="6.5in" height="2.688888888888889in"}

== Check Docker Setup 

From your Cloud9 IDE prompt, verify docker is configured correctly:

$ docker info

Containers: 1
Running: 0
Paused: 0
Stopped: 1
Images: 15
Server Version: 18.06.1-ce
Storage Driver: overlay2
Backing Filesystem: extfs
Supports d\_type: true
Native Overlay Diff: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins:
Volume: local
Network: bridge host macvlan null overlay
Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk
syslog
Swarm: inactive
Runtimes: runc
Default Runtime: runc
Init Binary: docker-init
containerd version: 468a545b9edcd5932818eb9de8e72413e616e86e
runc version: 69663f0bd4b60df09991c08812a60108003fa340
init version: fec3683
Security Options:
seccomp
Profile: default
Kernel Version: 4.14.121-85.96.amzn1.x86\_64
Operating System: Amazon Linux AMI 2018.03
OSType: linux
Architecture: x86\_64
CPUs: 1
Total Memory: 985.8MiB
Name: ip-172-31-93-202
ID: VSBK:S673:ZSWY:6MNN:CBIO:54NR:GWQK:LEIF:EBQH:JXND:FX3M:HRCW
Docker Root Dir: /var/lib/docker
Debug Mode (client): false
Debug Mode (server): false
Registry: https://index.docker.io/v1/
Labels:
Experimental: false
Insecure Registries:
127.0.0.0/8
Live Restore Enabled: false...

== Prepping the Docker images
-----------------------------

At this point, we're going to pretend that we're the developers of
both the web and api microservices, and we will get the latest from our
source repo. In this case we will just be using the plain old curl, but
just pretend you\'re using git:

$ curl -O
https://s3-us-west-2.amazonaws.com/apn-bootcamps/microservice-ecs-2017/ecs-lab-code-20170524.tar.gz

$ tar -xvf ecs-lab-code-20170524.tar.gz

Our first step is to build and test our containers locally. If you've
never worked with Docker before, there are a few basic commands that
we'll use in this workshop, but you can find a more thorough list in
the [Docker \"Getting Started\"
documentation](https://docs.docker.com/get-started/).

To build your first container, go to the web directory. This folder
contains our web Python Flask microservice:

$ cd /home/ec2-user/environment/aws-microservices-ecs-bootcamp-v2/web

To build the container:

$ docker build -t ecs-lab/web .

This should output steps that look something like this:

Sending build context to Docker daemon 4.096 kB
Sending build context to Docker daemon
Step 0 : FROM ubuntu:latest
\-\--\> 6aa0b6d7eb90
Step 1 : MAINTAINER widha\@amazon.com
\-\--\> Using cache
\-\--\> 3f2b91d4e7a9

If the container builds successfully, the output should end with
something like this:

Removing intermediate container d2cd523c946a
Successfully built ec59b8b825de

To view the image that was just built:

$ docker images

REPOSITORY TAG IMAGE ID CREATED SIZE

ecs-lab/web latest 0ba3cebe670c 42 seconds ago 435MB
utilities latest 3a74030b141d 13 minutes ago 289MB
ubuntu latest 7698f282e524 2 weeks ago 69.9MB
lambci/lambda python2.7 f81d6f78b2ff 2 weeks ago 682MB
lambci/lambda nodejs8.10 546cfb23364f 2 weeks ago 742MB
lambci/lambda python3.6 62cc52d59225 2 weeks ago 814MB
lambci/lambda nodejs4.3 3f2ef3c2a27c 2 weeks ago 712MB
lambci/lambda nodejs6.10 84e9cc454dd9 2 weeks ago 727MB
alpine 3.9.4 055936d39205 3 weeks ago 5.53MB

To run your container:

$ docker run -d -p 3000:3000 ecs-lab/web

This command runs the image in daemon mode and maps the docker container
port 3000 with the host (in this case our workstation) port 3000. We\'re
doing this so that we can run both microservices on a single host
without port conflicts.

To check if your container is running:

$ docker ps

This should return a list of all the currently running containers. In
this example, it should just return a single container, the one that we
just started:

CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES

7b0d04f4502c ecs-lab/web \"python app.py\" 9 seconds ago Up 9 seconds
0.0.0.0:3000-\>3000/tcp eloquent\_noether

To test the actual container output:

$ curl localhost:3000/web

This should return:

\<html\>\<head\>\...\</head\>\<body\>hi! i'm served via Python + Flask.
i\'m a web endpoint. \...\</body\>\</html\>

Repeat the same steps with the api microservice. Change directory to
/api and repeat the same steps above:

$ cd /home/ec2-user/environment/aws-microservices-ecs-bootcamp-v2/api
$ docker build -t ecs-lab/api .
$ docker images (should see the api image now)
$ docker run -d -p 8000:8000 ecs-lab/api
$ docker ps (should see both containers running)
$ curl localhost:8000/api

The API container should return:

{ \"response\" : \"hi! i\'m ALSO served via Python + Flask. i\'m an
API.\" }

**We now have two working microservice containers.**

== Creating container registries with ECR
-----------------------------------------

Once images are built, it's useful to share them and this is done by
pushing the images to a container registry. Let's create two
repositories in Amazon EC2 Container Registry
([ECR](https://aws.amazon.com/ecr/)).

Navigate to the [ECS console](https://console.aws.amazon.com/ecs/) and
Select ECR from AWS Services

![](media/image2.png){width="6.5in" height="3.504861111111111in"}

Next **Repositories** and choose **Create repository**.

![](media/image3.png){width="6.5in" height="1.2930555555555556in"}

Name your first repository **ecs-lab-web**:

![](media/image4.png){width="5.048611111111111in"
height="2.0874070428696414in"}

Once you\'ve created the ecs-lab-web repository, repeat the process for
the **ecs-lab-api** repository. Take note of the push commands for this
second repository. Push commands are unique per repository.

Once you\'ve created the repository, select it and then click View Push
Commands ![](media/image5.png){width="6.5in"
height="1.6868055555555554in"}

![](media/image6.png){width="6.5in" height="4.686111111111111in"}

== Configuring the AWS CLI
--------------------------

On our workstation, we will use the AWS CLI to push images to ECR. Let's
configure the CLI by running:

$ aws configure

This should drop you into a set of prompts. Since our workstation is an
EC2 instance pre-configured in an IAM role, the only information
required is your preferred region:

$ aws configure

AWS Access Key ID: \<leave empty\>
AWS Secret Access Key: \<leave empty\>
Default region name \[us-east-1\]: us-east-1
Default output format \[json\]: \<leave empty\>

You can confirm that your CLI is setup correctly by running the command
to obtain an ECR authentication token.

$ aws ecr get-login \--no-include-email \--region us-east-1

This should output something like:

docker login -u AWS -p
AQECAHhwm0YaISJeRtJm5n1G6uqeekXuoXXPe5UFce9Rq8/14wAAAy0wggMpBgkqhkiG9w0BBwagggMaMIIDFgIBADCCAw8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM+76slnFaYrrZwLJyAgEQgIIC4LJKIDmvEDtJyr7jO661//6sX6cb2jeD/RP0IA03wh62YxFKqwRMk8gjOAc89ICxlNxQ6+cvwjewi+8/W+9xbv5+PPWfwGSAXQJSHx3IWfrbca4WSLXQf2BDq0CTtDc0+payiDdsXdR8gzvyM7YWIcKzgcRVjOjjoLJpXemQ9liPWe4HKp+D57zCcBvgUk131xCiwPzbmGTZ+xtE1GPK0tgNH3t9N5+XA2BYYhXQzkTGISVGGL6Wo1tiERz+WA2aRKE+Sb+FQ7YDDRDtOGj4MwZ3/uMnOZDcwu3uUfrURXdJVddTEdS3jfo3d7yVWhmXPet+3qwkISstIxG+V6IIzQyhtq3BXW/I7pwZB9ln/mDNlJVRh9Ps2jqoXUXg/j/shZxBPm33LV+MvUqiEBhkXa9cz3AaqIpc2gXyXYN3xgJUV7OupLVq2wrGQZWPVoBvHPwrt/DKsNs28oJ67L4kTiRoufye1KjZQAi3FIPtMLcUGjFf+ytxzEPuTvUk4Xfoc4A29qp9v2j98090Qx0CHD4ZKyj7bIL53jSpeeFDh9EXubeqp6idIwG9SpIL9AJfKxY7essZdk/0i/e4C+481XIM/IjiVkh/ZsJzuAPDIpa8fPRa5Gc8i9h0bioSHgYIpMlRkVmaAqH/Fmk+K00yG8USOAYtP6BmsFUvkBqmRtCJ/Sj+MHs+BrSP7VqPbO1ppTWZ6avl43DM0blG6W9uIxKC9SKBAqvPwr/CKz2LrOhyqn1WgtTXzaLFEd3ybilqhrcNtS16I5SFVI2ihmNbP3RRjmBeA6/QbreQsewQOfSk1u35YmwFxloqH3w/lPQrY1OD+kySrlGvXA3wupq6qlphGLEWeMC6CEQQKSiWbbQnLdFJazuwRUjSQlRvHDbe7XQTXdMzBZoBcC1Y99Kk4/nKprty2IeBvxPg+NRzg+1e0lkkqUu31oZ/AgdUcD8Db3qFjhXz4QhIZMGFogiJcmo=
-e none https://\<account\_id\>.dkr.ecr.us-east-1.amazonaws.com

To register ECR as your Docker repository, copy and paste that output or
run:

$ aws ecr get-login \--region us-east-1

Your shell will execute the output of that command and respond:

Login Succeeded

If you are unable to login to ECR, check your IAM permissions.

== Pushing our tested images to ECR
-----------------------------------

Now that we've tested our images locally, we need to tag and push them
to ECR. This will allow us to use them in Task Definitions that can be
deployed to an ECS cluster.

You'll need your push commands that you saw during registry creation.
You can find them again by going back to the repository (**ECS Console**
\> **Repositories** \> Select the Repository you want to see the
commands for \> **View Push Commands)**.

To tag and push to the web repository (if you're using a shared account,
use your name in the tag: fred-ecs-lab:latest):

$ docker tag ecs-lab/web:latest
\<account\_id\>.dkr.ecr.us-east-1.amazonaws.com/ecs-lab-web:latest

$ docker push
\<account\_id\>.dkr.ecr.us-east-1.amazonaws.com/ecs-lab-web:latest

This should return something like this:

The push refers to a repository
\[\<account\_id\>.ecr.us-east-1.amazonaws.com/ecs-lab-web\] (len: 1)

ec59b8b825de: Image already exists
5158f10ac216: Image successfully pushed
860a4e60cdf8: Image successfully pushed
6fb890c93921: Image successfully pushed
aa78cde6a49b: Image successfully pushed
Digest:
sha256:fa0601417fff4c3f3e067daa7e533fbed479c95e40ee96a24b3d63b24938cba8

To tag and push to the api repository:

$ docker tag ecs-lab/api:latest
\<account\_id\>.dkr.ecr.us-east-1.amazonaws.com/ecs-lab-api:latest

$ docker push
\<account\_id\>.dkr.ecr.us-east-1.amazonaws.com/ecs-lab-api:latest

**Note**: why :latest? This is the actual image tag. In most production
environments, you\'d tag images for different schemes, for example, you
might tag the most up-to-date image with :latest, and all other versions
of the same container with a commit SHA from a CI job. If you push an
image without a specific tag, it will default to :latest, and untag the
previous image with that tag. For more information on Docker tags, see
the Docker
[documentation](https://docs.docker.com/engine/reference/commandline/tag/).

You can see your pushed images by viewing the repository in the [ECS
Console](https://console.aws.amazon.com/ecs). Alternatively, you can use
the CLI:

$ aws ecr list-images \--repository-name=ecs-lab-api

{

\"imageIds\": \[ {

\"imageTag\": \"latest\",

\"imageDigest\":
\"sha256:f0819d27f73c7fa6329644efe8110644e23c248f2f3a9445cbbb6c84a01e108f\"

}

\]

}

**You have successfully completed Lab 1.. Keep all the infrastructure
you have built running. You will be building on this in Lab 2**
