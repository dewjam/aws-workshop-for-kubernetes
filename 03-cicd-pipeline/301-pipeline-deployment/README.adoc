:icons:
:linkcss:
:imagesdir: ./images

= CodeSuite - Continuous Deployment Reference Architecture for Kubernetes

The CodeSuite Continuous Deployment reference architecture demonstrates how to achieve continuous
deployment of an application to a Kubernetes cluster using AWS CodePipeline, AWS CodeCommit, AWS CodeBuild and AWS Lambda.

Launching this AWS CloudFormation stack provisions a continuous deployment process that uses AWS CodePipeline
to monitor an AWS CodeCommit repository for new commits, AWS CodeBuild to create a new Docker container image and to push
it into Amazon ECR. Finally an AWS Lambda function with the Kubernetes Python SDK updates a Kubernetes deployment in a live cluster.

When you deploy the cloudformation stack there will be four parameters that are specific to your Kubernetes cluster. You will need the API endpoint (enter only the subdomain and omit 'api'), Certificate Authority Data, Client Certificate Data and Client Key Data.
The last of these three are sensitive, the cloudformation parameter is marked with the "NoEcho" property set to true so that the contents are not exposed through cloudformation. In addition those strings are encrypted with the account default
KMS key and stored in parameter store. The Lambda function that authenticates to your Kubernetes API endpoint is assigned an IAM role that has permission to access those keys. The Lambda function builds a config file in the tmpfs directory of the Lambda which is in memory
so that when the Lambda function terminates the secrets are gone.

image::architecture.png[Architecture]

=== Pre-Requisites

A functioning Kubernetes cluster and config file to authenticate to the cluster, by default this is located at `~/.kube/config`

=== Application - initial deployment and service Provisioning

We first deploy a simple nginx container as the first version of our service.

    kubectl apply -f aws-workshop-for-kubernetes/03-cicd-pipeline/301-pipeline-deployment/kube-manifests/deploy-first.yml

Find the service endpoint to view the application:

    kubectl get svc codesuite-demo -o wide

Updating the deployment will take a minute. If you copy and paste the "External IP" (actually an ELB DNS name) from the codesuite-demo service into a browser you should see the nginx homepage.

=== Create the CodePipeline

We now use a CloudFormation template to deploy the pipeline, along with its components (CodeCommit repository, CodeBuild project, Deployment step via Lambda). The pipeline will build a container and push the image to the ECR repo created in a previous lab.

Note, deploy this stack in the same region as your k8s cluster. Your cluster nodes will require access via an IAM profile to download images from ECR.

You need to specify 2 parameters for the stack (the rest are pre-populated):

 - the name of your EKS cluster
 - the name of the ECR repository you created in a previous lab for the API container

Also, make sure you specify a unique name for your stack.

|===

|Region | Launch Template for Amazon EKS
| *N. Virginia* (us-east-1)
a| image::./deploy-to-aws.png[link=https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=pipeline-YOUR-UNIQUE-NAME&templateURL=https://s3.amazonaws.com/cf-templates-16bq2bkk3lpm-us-east-1/ehi-aws-refarch-codesuite-kubernetes.yaml]

|===

Since you are deploying this architecture to an Amazon EKS cluster, you need to give the Lambda
execution role permissions in the Amazon EKS cluster. You can get the ARN of your Lambda execution role
from the Outputs tab in the nested CloudFormation template *pipeline-<your_unique_name>-Pipeline-<random_string>*. Refer to this 
link:https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html[User Guide] for detailed
instructions.

1. Edit the `aws-auth` ConfigMap of your cluster.

    kubectl -n kube-system edit configmap/aws-auth

2. Add your Lambda execution role to the config. The file you will edit will already contain a mapping for worker nodes. DO NOT edit the existing mapping, you need to add the second section below:

    # Please edit the object below. Lines beginning with a '#' will be ignored,
    # and an empty file will abort the edit. If an error occurs while saving this file will be
    # reopened with the relevant failures.
    #
    apiVersion: v1
    data:
      mapRoles: |
        - rolearn: arn:aws:iam::205675256514:role/devel-worker-nodes-NodeInstanceRole-74RF4UBDUKL6
          username: system:node:{{EC2PrivateDNSName}}
          groups:
            - system:bootstrappers
            - system:nodes
        #MAPPING BELOW IS WHAT YOU NEED TO ADD
        - rolearn: arn:aws:iam::205675256514:role/<your lambda execution role from stack output>
          username: admin
          groups:
            - system:masters

=== Test CI/CD platform

Install credential helper - this will let you access the CodeCommit repo created as part of the pipeline.

    git config --global credential.helper '!aws codecommit credential-helper $@'
    git config --global credential.UseHttpPath true

Change directories up one level `cd ..` so that both repositories are at the same directory level, then clone the CodeCommit Repository (url will be in the output of the nested CloudFormation template *pipeline-<your_unique_name>-Pipeline-<random_string>*).
Check the outputs page for your CloudFormation stack:

    git clone <url_of_your_codecommit_repository>

This creates a directory named `pipeline-<your_unique_name>` in your current directory.

We now want to push a new version of your API to the CodeCommit repository, to get it deployed to your EKS cluster, replacing your initial nginx deployment. Copy contents from aws-microservices-ecs-bootcamp-v2/api to this repository folder.

    rsync -av --exclude=".*" aws-microservices-ecs-bootcamp-v2/api/ <your_repository_directory>

Make a change to the response in the `app.py` file and then change into that directory `cd <your_repository_directory>`

Add, commit and push:

    git add . && git commit -m "test CodeSuite" && git push origin master

To view the pipeline in the AWS console go to the outputs tab for the pipeline cloudformation template and click on the Pipeline URL link:

image::pipeline-url.png[pipeline-url]

You can then see the pipeline move through the various stages:

image::pipeline.png[pipeline]

Once the final Lambda stage is complete you should be able to see the new deployment exposed through the same service load balancer.

    kubectl get svc codesuite-demo -o wide

Updating the deployment will take a minute. Once it's done, If you copy and paste the "External IP" (actually an ELB DNS name) from the codesuite-demo service into a browser you should see the flask page reflecting the changes you applied. Note that you will need to add "/api" at the end, since that is the path to the API you just deployed.

=== Cleaning up the example resources

To remove all resources created by this example do the following:

1. Delete the main CloudFormation stack which deletes the substacks and resources.
2. Manually delete resources which may contain files:
* S3 bucket: ArtifactBucket
* S3 bucket: LambdaCopy bucket
* ECR repository: Repository
3. Delete the Kubernetes deployment and service

== CloudFormation template resources

The following section explains all of the resources created the CloudFormation template provided with this example.

link:/templates/lambda-copy.yaml[lambda-copy]

This creates a Lambda function that copies the Lambda code from the central account into the user account.

link:/templates/ssm-inject.yaml[ssm-inject]

Deploys a custom resource via Lambda which creates secure string key value pairs for all of the secrets required to authenticate to the Kubernetes cluster.

link:/templates/deployment-pipeline.yaml[deployment-pipeline]

Resources that compose the deployment pipeline include the CodeBuild project, the CodePipeline pipeline, an S3 bucket for deployment artifacts, and ECR repository for the container images and all necessary IAM roles used by those services.

== License Summary

This sample code is made available under a modified MIT license. See the LICENSE file.
